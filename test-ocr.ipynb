{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "from multi_column import column_boxes\n",
    "import re\n",
    "doc = fitz.open('pima-two-col.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bro Optimizing Diabetes Mellitus Classification with\n",
      "Deep Neural Networks on the PIMA Indians Dataset\n",
      "Aadarsh Anantha Ramakrishnan, S Selvanayagam\n",
      "Department of Computer Science and Engineering\n",
      "National Institute of Technology, Tiruchirappalli\n",
      "Tamil Nadu, India, 620015\n",
      "{106121001, 106121133}@nitt.edu\n",
      "\n",
      "bro Abstract— Diabetes is a chronic illness that affects a sig-\n",
      "nificant number of individuals worldwide. The Pima Indi-\n",
      "ans, a group of indigenous people residing primarily in the\n",
      "southwestern region of the United States, including Arizona,\n",
      "have a higher incidence of diabetes than other populations.\n",
      "This study analyzes the Pima Indians Diabetes Dataset which\n",
      "contains diagnostic measurements of 768 Pima Indian women.\n",
      "The main aim of this paper was to create a Deep Neural\n",
      "Network-based classification model, capable of predicting\n",
      "diabetes among the Pima Indian population and compare that\n",
      "with the traditional methods used earlier for the same dataset.\n",
      "Our DNN model has been trained on this dataset and its per-\n",
      "formance achieves an accuracy rate of 91.34%, surpassing the\n",
      "performance of traditionally used classifiers such as K-Nearest\n",
      "Neighbors and Support Vector Machines. Our findings suggest\n",
      "that DNNs are a highly effective supervised learning algorithm\n",
      "for predicting diabetes in the Pima Indian population.\n",
      "Index Terms— PIMA, Deep Neural Networks, supervised\n",
      "learning algorithms, prediction\n",
      "I. INTRODUCTION\n",
      "High blood sugar levels are the stipulating feature of\n",
      "diabetes mellitus, a chronic metabolic condition caused\n",
      "by the body’s inability to make or utilise insulin. [1]. The\n",
      "disease can lead to a range of health complications such as\n",
      "cardiovascular disease, kidney failure, and stroke, among\n",
      "others [2]. Early detection and diagnosis of diabetes are\n",
      "crucial for improving patient outcomes and preventing\n",
      "long-term complications. In recent years, machine learn-\n",
      "ing (ML) has emerged as a powerful tool for predicting\n",
      "and diagnosing medical conditions, including diabetes [3].\n",
      "Deep Neural Networks (DNNs) are one such ML technique\n",
      "that has shown promising results in analyzing medical\n",
      "data and achieving high levels of accuracy in prediction\n",
      "tasks [4].\n",
      "The goal of this study is to predict diabetes in the\n",
      "PIMA Indian population using DNNs as a supervised\n",
      "learning algorithm. Prior research in this area has been\n",
      "heavily focused on using Support Vector Machines (SVMs)\n",
      "and K-Nearest Neighbor (KNN) algorithms. By utilizing\n",
      "DNNs, this study aims to enhance previous research by\n",
      "conducting a comparative analysis of the performance\n",
      "of various algorithms (SVM, KNN, XGBoost, and Logistic\n",
      "Regression) on the PIMA Indian Diabetes Dataset, and\n",
      "\n",
      "bro findings suggest that DNN outperforms the other four\n",
      "methods.\n",
      "To evaluate the functioning of our DNN model com-\n",
      "pared to other models, we used accuracy, precision, recall,\n",
      "and F1-score metrics. Our results show that DNNs can\n",
      "achieve a high level of accuracy in predicting diabetes\n",
      "in the PIMA Indian population, outperforming SVM, KNN\n",
      "and NB algorithms. These findings suggest that DNNs can\n",
      "be a valuable tool for predicting and diagnosing diabetes\n",
      "in other populations as well.\n",
      "The results of this research work show that DNNs can\n",
      "achieve a high level of accuracy in predicting diabetes in\n",
      "the PIMA Indian population, surpassing the performance\n",
      "of SVM and KNN algorithms. The model achieves an\n",
      "accuracy of 90% by employing DNNs in TensorFlow, an\n",
      "open-source library for data analysis and ML.\n",
      "Several studies have shown the effectiveness of DNNs\n",
      "in medical diagnosis and prediction tasks. For example,\n",
      "Dong et al. (2020) conducted a systematic review of the\n",
      "application of ML methods in diabetes prediction and\n",
      "found that DNNs are one of the most effective techniques\n",
      "for achieving high levels of accuracy [3]. In a compre-\n",
      "hensive review of deep learning techniques for medical\n",
      "diagnosis, Hassan et al. (2020) also reported the effec-\n",
      "tiveness of DNNs in various medical domains, including\n",
      "diabetes prediction [6]. Al-Turjman et al. (2019) reviewed\n",
      "intelligent health systems for the prediction and diagnosis\n",
      "of diabetes and found that DNNs are among the most\n",
      "promising techniques for achieving high accuracy [5].\n",
      "Overall, the findings of this research work suggest that\n",
      "DNNs can be an effective tool for predicting and diagnos-\n",
      "ing diabetes in other populations as well. The use of DNNs\n",
      "in medical diagnosis and prediction tasks is a promising\n",
      "area of research that can significantly improve patient\n",
      "outcomes and prevent long-term complications associated\n",
      "with chronic diseases like diabetes.\n",
      "This paper is organized as follows. Section 2 presents\n",
      "the related literary research on the proposed study. Sec-\n",
      "tion 3 presents the background and methodology of the\n",
      "proposed work, the dataset used, algorithms, and results.\n",
      "Section 4 analyzes the results and discussion. Section 5\n",
      "finally concludes the study.\n",
      "\n",
      "bro II. LITERATURE SURVEY\n",
      "\n",
      "bro A. Background\n",
      "Machine Learning (ML) is increasingly popular area\n",
      "of development in the field of healthcare. ML provides\n",
      "systems that learn from data and improve their behavior\n",
      "over time by automatically discovering emerging patterns\n",
      "from training datasets. The use of ML in healthcare can\n",
      "help improve patient outcomes and prevent long-term\n",
      "complications associated with chronic diseases like dia-\n",
      "betes. In this study, we have focused on using supervised\n",
      "learning models for the classification of diabetes risk\n",
      "prediction. The most common classification methods used\n",
      "in this study include Logistic Regression, Support Vector\n",
      "Machines (SVM), K Nearest Neighbours (KNN), XGBoost\n",
      "and Deep Neural Networks (DNN). In particular, we are\n",
      "interested in comparing the performance of these algo-\n",
      "rithms with Deep Neural Networks (DNNs) for predicting\n",
      "diabetes in the PIMA Indian population. Our study aims to\n",
      "determine if DNNs can achieve a higher level of accuracy\n",
      "in predicting diabetes than existing state-of-the-art algo-\n",
      "rithms like SVM, KNN, XGBoost, and Logistic Regression.\n",
      "The results of this study provides us with valuable insights\n",
      "into the use of DNNs for medical diagnosis and predic-\n",
      "tion tasks. They may lead to significant improvements in\n",
      "patient outcomes and healthcare in general.\n",
      "B. Previous Studies\n",
      "The PIMA Indian Diabetes Dataset [7] has been the\n",
      "subject of much research in the field of machine learning,\n",
      "with various methods proposed to achieve accurate clas-\n",
      "sification results. Deng and Kasabov [8] employed cross-\n",
      "validation and Self-Organizing Maps (SOM) to achieve\n",
      "78.4% accuracy in their classification of the PIMA dataset.\n",
      "Yu et al. [9] explored a combination of methods, including\n",
      "Quantum Particle Swarm Optimization (QPSO), Weighted\n",
      "Least Square (WLS) Support Vector Machine (SVM), and\n",
      "Neural Network, to achieve a classification accuracy of up\n",
      "to 82.18% with the WLS-SVM method. Al Jarullah et al.\n",
      "[10] utilized the c4.5 algorithm and achieved an accuracy\n",
      "of 71.1\n",
      "Pasi Lukka [11] used a combination of feature selection\n",
      "methods based on fuzzy entropy measures and similarity\n",
      "classifiers, achieving a classification accuracy of 75.29%.\n",
      "Seera et al. [12] proposed a hybrid intelligent system that\n",
      "combines the Fuzzy Min-Max neural network, decision\n",
      "tree, and Random Forest model. This approach achieved\n",
      "71.35% accuracy and provided incremental learning fea-\n",
      "tures by incorporating the neural network model and the\n",
      "ability to explain the decision process by incorporating\n",
      "the decision tree.\n",
      "In another paper, Choubey [13] proposed an approach\n",
      "using Genetic Algorithm (GA) for feature selection and\n",
      "Naive Bayes (NB) for classification, achieving an accuracy\n",
      "of 78.69%. . Kumari et al. [14] applied the SVM model for\n",
      "PIMA classification and investigated the performance of\n",
      "various kernels in their experiments. The paper reported\n",
      "\n",
      "bro a classification accuracy of 75.50% using RBF kernel\n",
      "and cross-validation method to tune the SVM hyper-\n",
      "parameters. Somu et al. [15] introduced RSKHT (Rough\n",
      "Set based K–Helly) feature selection technique and com-\n",
      "bined it with the Random Forest classification method,\n",
      "achieving 75.02%, 73.11%, 75.11%, and 74.9% accuracy in\n",
      "their experiments with Random Forest, Bayesian Network,\n",
      "Neural Network, and Decision Trees respectively.\n",
      "A comparative study of these methods reveals that they\n",
      "achieve varying degrees of accuracy in PIMA classification.\n",
      "While Yu et al. [9] achieved the highest accuracy of 82.18%,\n",
      "other approaches such as those of Choubey et al. [13]\n",
      "using GA feature selection achieved an accuracy of 78.69%.\n",
      "Meanwhile, Seera et al. [12] achieved 71.35% accuracy\n",
      "using a hybrid intelligent system, and Al Jarullah et al. [10]\n",
      "reported an accuracy of 71.1% using the c4.5 algorithm.\n",
      "Overall, the use of machine learning methods such\n",
      "as SVM, Random Forest, decision trees, and the neural\n",
      "network has proven effective in PIMA classification. While\n",
      "each method has its own strengths and weaknesses, the\n",
      "development of hybrid intelligent systems and the appli-\n",
      "cation of feature selection techniques have shown promise\n",
      "in achieving even greater accuracy in the future.\n",
      "III. METHODOLOGY\n",
      "A. PIMA Indians Diabetes Dataset\n",
      "1) Data\n",
      "Description:\n",
      "The\n",
      "PIMA\n",
      "Indians\n",
      "Diabetes\n",
      "dataset (PIDD) [7] contains 8 features, such as Pregnancy\n",
      "History, Glucose Levels, Blood Pressure, Skin Thickness,\n",
      "Insulin, BMI, Diabetic Pedigree Function, and Age. There\n",
      "are more instances of non-diabetic (labeled 0) individuals\n",
      "(500) than diabetic (labeled 1) ones (268) as shown in\n",
      "Figure 1. Sample data from the Pima Indians Dataset has\n",
      "been shown in Figure 2.\n",
      "Fig. 1.\n",
      "Outcomes Distribution in the dataset\n",
      "2) Data Cleaning: One of the biggest challenges with\n",
      "this dataset, is the presence of missing data in the Glucose,\n",
      "Blood Pressure, Skin Thickness, Insulin and BMI features.\n",
      "To combat this issue, this study uses the following steps:\n",
      "\n",
      "bro • Step 1: Replace the value with NaN for each feature,\n",
      "to indicate the presence of a missing value.\n",
      "• Step 2: Calculate the median value by outcome for\n",
      "each feature, and replace NaN with this median value.\n",
      "• Step 3: Check the values of all features after the above\n",
      "adjustment, to ensure the completeness of data.\n",
      "3) Feature Engineering: In this study, we create new\n",
      "features to result in faster training and more accurate\n",
      "predictions while building a model. The following features\n",
      "were newly created after analyzing the dataset:\n",
      "Feature\n",
      "Feature Description\n",
      "N0\n",
      "BMI * Skin Thickness\n",
      "N1\n",
      "Glucose <= 120 and Age <= 30\n",
      "N2\n",
      "BMI <= 30\n",
      "N3\n",
      "Age <= 30 and Pregnancies <= 6\n",
      "N4\n",
      "Glucose <= 105 and Blood Pressure <= 80\n",
      "N5\n",
      "Skin Thickness <= 20\n",
      "N6\n",
      "BMI < 30 and Skin Thickness <= 20\n",
      "N7\n",
      "Glucose <= 105 and BMI <= 30\n",
      "N8\n",
      "Pregnancies / Age\n",
      "N9\n",
      "Insulin < 200\n",
      "N10\n",
      "Blood Pressure < 80\n",
      "N11\n",
      "Pregnancies > 0 and < 4\n",
      "N12\n",
      "Age * Diabetes Pedigree Function\n",
      "N13\n",
      "Glucose / Diabetes Pedigree Function\n",
      "N14\n",
      "Age / Insulin\n",
      "N0, N8, N12, N13, and N14 are features that have been\n",
      "created, to boost the relevant features in the dataset. All\n",
      "the other features have been created, by plotting each\n",
      "feature and figuring out a general trend from the plot.\n",
      "4) Data Standardization: After the above process, it is\n",
      "essential to standardize the dataset as it is a common\n",
      "requirement for various machine learning estimators to\n",
      "perform well for the given dataset. This study utilized\n",
      "the Standard Scaler algorithm to standardize the data, to\n",
      "evaluate the accuracy of different algorithms. The dataset\n",
      "after standardization has been presented in Figure 3.\n",
      "B. ML Techniques for Prediction of Diabetes\n",
      "In this section, different supervised learning methods\n",
      "are presented for classifying Pima Indian women as either\n",
      "diabetic or non-diabetic. These methods use the original\n",
      "dataset to create separate training and testing datasets to\n",
      "accurately classify or predict diabetes.\n",
      "1) Support Vector Machine: Support Vector Machines\n",
      "(SVMs) have been extensively researched and studied in\n",
      "the field of machine learning. The original idea behind\n",
      "SVMs was introduced by Vapnik and Chervonenkis in 1963\n",
      "[16]. The modern version of SVMs, which incorporates the\n",
      "kernel trick, was developed by Boser, Guyon, and Vapnik\n",
      "in 1992 [17]. Since then, SVMs have been widely used\n",
      "in various applications such as image classification, text\n",
      "classification, and bioinformatics.\n",
      "\n",
      "bro The concept of the kernel trick was initially proposed by\n",
      "Aizerman et al. in 1964 [18], and it was later formalized\n",
      "by Mercer in 1909 [19]. The kernel trick enables SVMs\n",
      "to implicitly map the data to be analysed into a high-\n",
      "dimensional feature space where linear separation is pos-\n",
      "sible.\n",
      "In summary, SVMs are a powerful and widely used\n",
      "machine\n",
      "learning\n",
      "technique\n",
      "that\n",
      "can\n",
      "handle\n",
      "high-\n",
      "dimensional and non-linear datasets while being less\n",
      "susceptible to overfitting.\n",
      "2) K-Nearest Neighbours: K-nearest neighbors (KNN) is\n",
      "a non-parametric algorithm used for classification and\n",
      "regression tasks. It works by finding the K-nearest neigh-\n",
      "bors of a data point and assigning it to the class with\n",
      "the majority vote among those neighbors. In other words,\n",
      "KNN identifies the K closest data points in the training\n",
      "set to a given test point and then classifies the test point\n",
      "by the most common class among its K nearest neighbors\n",
      "[20].\n",
      "KNN is a powerful algorithm for classification tasks,\n",
      "particularly when the decision boundary is complex or\n",
      "the data is not linearly separable [21]. KNN is considered\n",
      "a lazy learner, as it does not work by making assumptions\n",
      "on the underlying data distribution and instead relies on\n",
      "the training data for classification. [22]\n",
      "Overall, KNN is a useful algorithm for classification\n",
      "tasks, particularly when the decision boundary is complex\n",
      "or the data is not linearly separable. Its simplicity and\n",
      "interpretability make it a popular choice among data\n",
      "scientists, though it does have some limitations that\n",
      "should be considered when applying the algorithm to\n",
      "real-world datasets.\n",
      "3) XGBoost:\n",
      "XGBoost is a popular machine-learning\n",
      "algorithm that has gained significant attention and is\n",
      "considered as a state-of-the-art algorithm for classification\n",
      "and regression tasks. It is based on the gradient boosting\n",
      "decision tree (GBDT) algorithm [23]. XGBoost enhances\n",
      "the performance of GBDT by employing a variety of tech-\n",
      "niques such as parallel computing on a single machine,\n",
      "effective regularization, and handling missing values [24].\n",
      "One of the main strengths of XGBoost is its ability to\n",
      "handle high-dimensional data, which is a common char-\n",
      "acteristic of many real-world datasets. In particular, it has\n",
      "been shown to perform well on image and text data [25].\n",
      "The algorithm also employs an effective regularization\n",
      "technique that reduces overfitting, which is a common\n",
      "problem with high-dimensional data. This technique is\n",
      "based on L1 and L2 regularization and can be controlled\n",
      "by adjusting the hyperparameters of the model [24].\n",
      "Another advantage of XGBoost is its ability to handle\n",
      "missing values. This is particularly useful in medical\n",
      "datasets like the PIMA dataset, where missing values are\n",
      "common. XGBoost uses a technique called approximate\n",
      "greedy coordinate descent to impute missing values [24].\n",
      "In summary, XGBoost is a powerful machine-learning\n",
      "\n",
      "bro Fig. 2.\n",
      "Pima Indians Diabetes Dataset\n",
      "Fig. 3.\n",
      "Dataset after Feature Engineering and Standardization\n",
      "\n",
      "bro algorithm that can handle high-dimensional data, has ef-\n",
      "fective regularization techniques, and can handle missing\n",
      "values. Its ability to perform parallel computing on a\n",
      "single machine and its compatibility with feature selection\n",
      "techniques and other algorithms make it a suitable choice\n",
      "for many real-world classification and regression tasks.\n",
      "4) Logistic Regression: Logistic regression is a popular\n",
      "machine learning algorithm used for binary classification\n",
      "tasks. It is a statistical model that uses a logistic function\n",
      "to model the probability of a binary response variable\n",
      "based on one or more predictor variables [26]. Logistic\n",
      "regression has been widely used in various applications,\n",
      "including finance, healthcare, and marketing, due to its\n",
      "simplicity, interpretability, and effectiveness [27].\n",
      "One of the strengths of logistic regression is its ability\n",
      "to handle both continuous and categorical predictor vari-\n",
      "ables. Moreover, logistic regression provides interpretable\n",
      "coefficients, which can be used to understand the im-\n",
      "portance of each predictor variable in the model. This\n",
      "interpretability also enables domain experts to validate the\n",
      "model and make informed decisions based on the model\n",
      "outputs. [27]\n",
      "One major advantage of using Logistic Regression is\n",
      "that it can handle high-dimensional data and perform\n",
      "well in sparse datasets [28]. This makes logistic regression\n",
      "a suitable choice for problems with a large number of\n",
      "features or when the dataset has missing values.\n",
      "Furthermore, logistic regression can be easily extended\n",
      "to handle multi-class classification tasks using techniques\n",
      "such as one-vs-all and softmax regression [29]. In addition,\n",
      "logistic regression can be combined with other ML algo-\n",
      "\n",
      "bro rithms such as decision trees and support vector machines\n",
      "to improve the model’s performance [30].\n",
      "It can be used as a baseline algorithm for the PIMA\n",
      "Indian Diabetes Dataset, and its performance can be\n",
      "further improved by combining it with feature selection\n",
      "techniques or other machine learning algorithms.\n",
      "C. Proposed Method\n",
      "Artificial Neural Network (ANN) is a model that is\n",
      "inspired by the functioning and structure of biological\n",
      "neurons. A neural network is a connection of multiple\n",
      "neurons connected as the human brain is a connection\n",
      "of 86 billion biological neurons. Along with this, an ANN\n",
      "consists of one or more hidden layers that process the\n",
      "information through neurons and each node works as\n",
      "an activation node; it classifies the outcome of artificial\n",
      "neurons for a better outcome.\n",
      "A dense neural network is a type of artificial neural\n",
      "network where each neuron is connected to every neuron\n",
      "in the adjacent layers. In other words, all the neurons\n",
      "are connected from one layer to the next layer. Each\n",
      "connection between neurons has a corresponding weight,\n",
      "which determines the strength of the connection. [31]\n",
      "Each neuron in a neural network has an activation\n",
      "function, which is used to determine the importance of an\n",
      "output from the neuron. This paper uses the Parametric\n",
      "Rectified Linear Unit (PReLU) and Sigmoid activation\n",
      "function. PReLU is a modification of the Rectified Linear\n",
      "Unit (ReLU) activation which sets all negative values to\n",
      "zero and leaves positive values unchanged. PReLU intro-\n",
      "duces a parameter that allows it to adjust the slope of\n",
      "\n",
      "bro the function for negative inputs. This means that PReLU\n",
      "can learn the appropriate slope for negative inputs during\n",
      "training. The equation of PReLU is described in 1.\n",
      "\n",
      "bro samples. The F1 score is interpreted as the harmonic\n",
      "mean of precision and recall, and an F1 score has its best\n",
      "value at 1 and the worst score at 0.\n",
      "\n",
      "bro f (x) =\n",
      "\n",
      "bro x,\n",
      "x > 0\n",
      "αx,\n",
      "x ≤ 0\n",
      "(1)\n",
      "\n",
      "bro Accuracy =\n",
      "T P +T N\n",
      "T P +T N +FP +F N\n",
      "(4)\n",
      "\n",
      "bro Precision =\n",
      "T P\n",
      "T P +FP\n",
      "(5)\n",
      "\n",
      "bro where α is the learnable parameter that determines the\n",
      "slope for negative inputs and x is the input feature.\n",
      "The Sigmoid activation is a mathematical function that\n",
      "maps any input value to a value between 0 and 1. It is also\n",
      "a non-linear function, which allows neural networks to\n",
      "model complex relationships between inputs and outputs.\n",
      "The function has been described in 2, where x is the input\n",
      "feature.\n",
      "f (x) =\n",
      "1\n",
      "1+e−x\n",
      "(2)\n",
      "\n",
      "bro where α is the learnable parameter that determines the\n",
      "slope for negative inputs and x is the input feature.\n",
      "The Sigmoid activation is a mathematical function that\n",
      "maps any input value to a value between 0 and 1. It is also\n",
      "a non-linear function, which allows neural networks to\n",
      "model complex relationships between inputs and outputs.\n",
      "The function has been described in 2, where x is the input\n",
      "feature.\n",
      "f (x) =\n",
      "1\n",
      "1+e−x\n",
      "(2)\n",
      "\n",
      "bro Recall =\n",
      "T P\n",
      "T P +F N\n",
      "(6)\n",
      "\n",
      "bro F1 = 2∗Precision ∗Recall\n",
      "Precision +Recall\n",
      "(7)\n",
      "where TP = True Positives, FP = False Positives, TN = True\n",
      "Negatives and FN = False Negatives\n",
      "IV. RESULTS\n",
      "In this paper, we compared the performance of five\n",
      "classification algorithms (XGBoost, Logistic Regression,\n",
      "KNN, SVM and DNN) in predicting diabetes using the\n",
      "PIMA dataset. The dataset has been split into 70% training\n",
      "data and 30% testing data, and used for model evalutation.\n",
      "The DNN has been trained with the BinaryCrossentropy\n",
      "Loss Function and uses the Adam Optimizer for finding\n",
      "an optimal learning rate.\n",
      "Our results showed that the DNN outperformed the\n",
      "other methods, achieving the highest accuracy of 91.34%.\n",
      "The second-best method was XGBoost, with an accu-\n",
      "racy of 89.61%. Logistic Regression achieved an accuracy\n",
      "of 83.55%, while KNN and SVM achieved accuracies of\n",
      "80.08% and 79.65%, respectively.\n",
      "TABLE I\n",
      "ACCURACY, PRECISION AND RECALL COMPARISON OF DIFFERENT METHODS\n",
      "Method\n",
      "Accuracy\n",
      "Precision\n",
      "Recall\n",
      "F1\n",
      "XGBoost\n",
      "89.61\n",
      "0.8382\n",
      "0.8142\n",
      "0.8260\n",
      "SVM Classifier\n",
      "79.65\n",
      "0.6455\n",
      "0.7285\n",
      "0.6845\n",
      "KNN Classifier\n",
      "80.08\n",
      "0.6875\n",
      "0.6285\n",
      "0.6567\n",
      "Logistic Regression\n",
      "83.55\n",
      "0.7500\n",
      "0.6857\n",
      "0.7164\n",
      "Deep Neural Network\n",
      "91.34\n",
      "0.8315\n",
      "0.9367\n",
      "0.8934\n",
      "The superior performance of DNN can be attributed to\n",
      "its ability to learn complex and nonlinear relationships\n",
      "between features and target variables. DNN can auto-\n",
      "matically extract high-level features from raw input data\n",
      "and use them to make accurate predictions. In contrast,\n",
      "the other methods we tested rely on linear relationships\n",
      "between features and target variables, which may not\n",
      "be sufficient for capturing the complexity of the PIMA\n",
      "dataset.\n",
      "V. CONCLUSIONS\n",
      "In conclusion, the results of this study illustrate the\n",
      "potential of using DNNs for predicting diabetes in the\n",
      "PIMA Indian population. Our findings indicate that DNNs\n",
      "outperform other traditional algorithms such as SVM,\n",
      "KNN, XGBoost, and Logistic Regression in terms of accu-\n",
      "racy, precision, recall, and F1-score metrics. The accuracy\n",
      "\n",
      "bro We have also implemented Lasso Regression (Least\n",
      "Absolute Shrinkage and Selection Operator) or L1 regu-\n",
      "larization which adds the “absolute value of magnitude”\n",
      "coefficient as a regularization term to the loss function\n",
      "to avoid underfitting. The coefficient can be calculated\n",
      "using the function described in 3. Another merit of Lasso\n",
      "Regression is that it shrinks the less significant parameters\n",
      "to zero to train the model with the most important\n",
      "parameters.\n",
      "\n",
      "bro Loss = Error(Y − �Y )+λ\n",
      "n�\n",
      "1\n",
      "\n",
      "bro |wi|\n",
      "(3)\n",
      "\n",
      "bro where Y is the actual label, �Y is the predicted label, λ\n",
      "is the regularization parameter.\n",
      "In this paper, we propose a Dense Neural Network\n",
      "consisting of an input layer, 2 Dense layers with a PReLU\n",
      "activation function and a final output layer with a Sigmoid\n",
      "activation function. The input layer consists of 24 neurons,\n",
      "the first dense layer consists of 100 neurons, the second\n",
      "dense layer consists of 50 neurons and the output layer\n",
      "has a single neuron. The overall architecture of the neural\n",
      "network can be found in Fig. 4. The L1 regularization\n",
      "technique (Lasso Regression) has been implemented to\n",
      "create a less complex model and to address over-fitting\n",
      "issues. A flowchart of our proposed method has been\n",
      "presented in Fig. 5.\n",
      "D. Performance Evaluation\n",
      "To evaluate different models, we will be using the\n",
      "Accuracy, Precision, Recall and F1 metrics. Accuracy can\n",
      "be defined as the percentage of correct predictions done\n",
      "by the model. It can also be defined as the ratio between\n",
      "the sum of True Positives (TP) and True Negatives (TN) to\n",
      "the count of the entire dataset. Precision can be defined as\n",
      "the ratio of correctly classified positive samples (true pos-\n",
      "itives) to the total number of classified positive samples.\n",
      "The recall is calculated by the ratio between the number\n",
      "of positive samples (true positives) correctly classified\n",
      "to the total number of positive samples (true positives).\n",
      "The recall measures the model’s ability to detect positive\n",
      "\n",
      "bro Fig. 4.\n",
      "DNN Architecture\n",
      "Fig. 5.\n",
      "Flowchart of our Proposed Model\n",
      "\n",
      "bro of the DNN model achieved in this study (91.34%) is highly\n",
      "promising and can be used as a tool for predicting and\n",
      "diagnosing diabetes in other populations as well.\n",
      "The utilization of DNNs proves to be a powerful ap-\n",
      "proach in the field of ML and data analysis. However, it\n",
      "is imperative that there is always room for further im-\n",
      "provement and optimization of the DNN model to achieve\n",
      "even higher levels of accuracy. Additionally, the ethical\n",
      "implications of using predictive models in healthcare and\n",
      "\n",
      "bro ensure that these models do not perpetuate biases or\n",
      "discrimination.\n",
      "In conclusion, this study contributes to the growing\n",
      "body of research on the application of ML techniques\n",
      "in healthcare and provides evidence for the potential of\n",
      "DNNs as a tool for predicting and diagnosing diabetes.\n",
      "Future research could explore the use of DNNs in other\n",
      "healthcare domains and investigate the impact of such\n",
      "models on clinical decision-making and patient outcomes.\n",
      "\n",
      "bro REFERENCES\n",
      "[1] J. B. Buse, \"Diabetes mellitus,\" in Harrison’s Principles of Internal\n",
      "Medicine, 20th ed., D. L. Longo, Ed. New York, NY, USA: McGraw-\n",
      "Hill Education, 2018, ch. 342, pp. 2395-2410.\n",
      "[2] World Health Organization. (2021). Diabetes. Retrieved from https:\n",
      "//www.who.int/news-room/fact-sheets/detail/diabetes\n",
      "[Accessed 27 Mar. 2023].\n",
      "[3] Dong, Y., Xie, M., Jiang, X., & Yuan, Y. (2020). Application of\n",
      "machine learning methods in diabetes prediction: A systematic\n",
      "review. Frontiers in Endocrinology, 11, 1-14. Available: https://\n",
      "doi.org/10.3389/fendo.2020.578605 [Accessed 27 Mar. 2023].\n",
      "[4] Hassan, A. R., Mabrouk, M. S.,& Zaki, W. M. (2020). A compre-\n",
      "hensive review of deep learning techniques for medical diagnosis.\n",
      "Journal of Healthcare Engineering, 2020, 1-22. Available: https:\n",
      "//doi.org/10.1155/2020/8887491 [Accessed 27 Mar. 2023].\n",
      "[5] Smith, J. W., Everhart, J. E., Dickson, W. C., Knowler, W. C., &\n",
      "Johannes, R. S. (1988). Using the ADAP learning algorithm to\n",
      "forecast the onset of diabetes mellitus. In Proceedings of the Annual\n",
      "Symposium on Computer Application in Medical Care (pp. 261-\n",
      "265). Available: https://pubmed.ncbi.nlm.nih.gov/3243031/\n",
      "[Accessed 27 Mar. 2023].\n",
      "[6] Al-Turjman, F. (2019). Intelligent health systems for prediction and\n",
      "diagnosis of diabetes: A review. Journal of Healthcare Engineering,\n",
      "2019, 1-13. Available: https://doi.org/10.1155/2019/7061086\n",
      "[Accessed 27 Mar. 2023]..\n",
      "[7] UC Irvine Machine Learning Repository. (n.d.). Pima Indians Di-\n",
      "abetes Database. [Online]. Available: https://www.kaggle.com/\n",
      "datasets/uciml/pima-indians-diabetes-database [accessed\n",
      "28 Mar. 2023].\n",
      "\n",
      "bro [20] Alpaydin, E. (2010). Introduction to machine learning (2nd ed.).\n",
      "MIT Press.\n",
      "[21] Hastie, T., Tibshirani, R., & Friedman, J. (2009). The elements of\n",
      "statistical learning: Data mining, inference, and prediction (2nd\n",
      "ed.). Springer.\n",
      "[22] Bramer, M. (2015). Principles of data mining (3rd ed.). Springer.\n",
      "[23] Friedman, J. H. (2001). Greedy function approximation: A gradient\n",
      "boosting machine. Annals of statistics, 1189-1232.\n",
      "[24] Chen, T., & Guestrin, C. (2016). Xgboost: A scalable tree boosting\n",
      "system. In Proceedings of the 22nd acm sigkdd international con-\n",
      "ference on knowledge discovery and data mining (pp. 785-794).\n",
      "[25] Huang, J., Li, Z., & Hu, H. (2019). Xgboost for multi-view learning\n",
      "of image and text data. IEEE Access, 7, 26139-26146.\n",
      "[26] Hosmer Jr, D. W., Lemeshow, S., & Sturdivant, R. X. (2013). Applied\n",
      "logistic regression (Vol. 398). John Wiley & Sons.\n",
      "[27] Agresti, A. (2002). Categorical data analysis (Vol. 482). John Wiley\n",
      "& Sons.\n",
      "[28] Peduzzi, P., Concato, J., Kemper, E., Holford, T. R., & Feinstein, A. R.\n",
      "(1996). A simulation study of the number of events per variable in\n",
      "logistic regression analysis. Journal of clinical epidemiology, 49(12),\n",
      "1373-1379.\n",
      "[29] Bishop, C. M. (2006). Pattern recognition and machine learning (Vol.\n",
      "4). Springer.\n",
      "[30] Hastie, T., Tibshirani, R., & Friedman, J. (2009). The elements of\n",
      "statistical learning: data mining, inference, and prediction (2nd ed.).\n",
      "Springer.\n",
      "[31] I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning. MIT Press,\n",
      "2016.\n",
      "\n",
      "bro [8] Deng, L. and Kasabov, N., \"Feature selection and classification\n",
      "of Pima Indians diabetes dataset using self-organizing maps and\n",
      "cross-validation,\" International Journal of General Systems, vol. 43,\n",
      "no. 4, pp. 375-391, 2014.\n",
      "[9] Yu, W., Liu, T., Valdez, R., Gao, J. and Zeng, X., \"Quantum particle\n",
      "swarm optimization and weighted least squares support vector\n",
      "machine for Pima Indian diabetes classification,\" Computational\n",
      "and Mathematical Methods in Medicine, vol. 2013, Article ID\n",
      "398781, 11 pages, 2013.\n",
      "[10] Al Jarullah, A., Fakhrzadeh, M., and Asadi-Shekari, Z., \"An empirical\n",
      "comparison of different decision tree algorithms for predicting\n",
      "diabetes mellitus,\" International Journal of Healthcare Information\n",
      "Systems and Informatics (IJHISI), vol. 10, no. 2, pp. 18-38, 2015.\n",
      "[11] Lukka, P., \"Fuzzy entropy measures based feature selection for Pima\n",
      "Indians diabetes dataset classification,\" International Journal of\n",
      "Computer Applications, vol. 55, no. 10, pp. 38-42, 2012.\n",
      "[12] Seera, M., Yoo, P.D. and Kim, D.H., \"An intelligent system for the\n",
      "diagnosis of diabetes mellitus using FMM neural network and\n",
      "decision tree,\" Journal of Medical Systems, vol. 36, no. 5, pp. 3135-\n",
      "3144, 2012.\n",
      "[13] Choubey, S., \"PIMA Indian diabetes dataset classification using\n",
      "Naive Bayes and Genetic Algorithm based feature selection,\" In-\n",
      "ternational Journal of Computer Applications, vol. 95, no. 6, pp.\n",
      "1-8, 2014.\n",
      "[14] Kumari, A., Kumar, A. and Kumar, V., \"Classification of Pima Indians\n",
      "diabetes data using support vector machines,\" International Journal\n",
      "of Computer Science Issues, vol. 10, no. 2, pp. 20-26, 2013.\n",
      "[15] Somu, R., Lakshmanaprabu, S.K., and Kannan, A., \"Enhanced pre-\n",
      "diction of diabetes using rough set based K-Helly feature selection\n",
      "algorithm with random forest classifier,\" International Journal of\n",
      "Computer Applications, vol. 103, no. 4, pp. 19-24, 2014.\n",
      "[16] Vapnik, V. N., & Chervonenkis, A. Ya. (1963). On the uniform\n",
      "convergence of relative frequencies of events to their probabilities.\n",
      "Theory of Probability & Its Applications, 8(2), 288-29\n",
      "[17] Boser, B. E., Guyon, I. M., & Vapnik, V. N. (1992). A training\n",
      "algorithm for optimal margin classifiers. Proceedings of the fifth\n",
      "annual workshop on Computational learning theory, 144-152.\n",
      "[18] Aizerman, M. A., Braverman, E. M., & Rozonoer, L. I. (1964).\n",
      "Theoretical foundations of the potential function method in pattern\n",
      "recognition learning. Automation and Remote Control, 25(6), 821-\n",
      "837.\n",
      "[19] Mercer, J. (1909). Functions of positive and negative type, and\n",
      "their connection with the theory of integral equations. Philo-\n",
      "sophical Transactions of the Royal Society A, 209, 415-446. doi:\n",
      "10.1098/rsta.1909.0013\n",
      "\n",
      "Optimizing Diabetes Mellitus Classification with\n",
      "Deep Neural Networks on the PIMA Indians Dataset\n",
      "Aadarsh Anantha Ramakrishnan, S Selvanayagam\n",
      "Department of Computer Science and Engineering\n",
      "National Institute of Technology, Tiruchirappalli\n",
      "Tamil Nadu, India, 620015\n",
      "{106121001, 106121133}@nitt.edu\n",
      "Abstract Diabetes is a chronic illness that affects a sig-\n",
      "nificant number of individuals worldwide. The Pima Indi-\n",
      "ans, a group of indigenous people residing primarily in the\n",
      "southwestern region of the United States, including Arizona,\n",
      "have a higher incidence of diabetes than other populations.\n",
      "This study analyzes the Pima Indians Diabetes Dataset which\n",
      "contains diagnostic measurements of 768 Pima Indian women.\n",
      "The main aim of this paper was to create a Deep Neural\n",
      "Network-based classification model, capable of predicting\n",
      "diabetes among the Pima Indian population and compare that\n",
      "with the traditional methods used earlier for the same dataset.\n",
      "Our DNN model has been trained on this dataset and its per-\n",
      "formance achieves an accuracy rate of 91.34%, surpassing the\n",
      "performance of traditionally used classifiers such as K-Nearest\n",
      "Neighbors and Support Vector Machines. Our findings suggest\n",
      "that DNNs are a highly effective supervised learning algorithm\n",
      "for predicting diabetes in the Pima Indian population.\n",
      "Index Terms PIMA, Deep Neural Networks, supervised\n",
      "learning algorithms, prediction\n",
      "I. INTRODUCTION\n",
      "High blood sugar levels are the stipulating feature of\n",
      "diabetes mellitus, a chronic metabolic condition caused\n",
      "by the bodys inability to make or utilise insulin. [1]. The\n",
      "disease can lead to a range of health complications such as\n",
      "cardiovascular disease, kidney failure, and stroke, among\n",
      "others [2]. Early detection and diagnosis of diabetes are\n",
      "crucial for improving patient outcomes and preventing\n",
      "long-term complications. In recent years, machine learn-\n",
      "ing (ML) has emerged as a powerful tool for predicting\n",
      "and diagnosing medical conditions, including diabetes [3].\n",
      "Deep Neural Networks (DNNs) are one such ML technique\n",
      "that has shown promising results in analyzing medical\n",
      "data and achieving high levels of accuracy in prediction\n",
      "tasks [4].\n",
      "The goal of this study is to predict diabetes in the\n",
      "PIMA Indian population using DNNs as a supervised\n",
      "learning algorithm. Prior research in this area has been\n",
      "heavily focused on using Support Vector Machines (SVMs)\n",
      "and K-Nearest Neighbor (KNN) algorithms. By utilizing\n",
      "DNNs, this study aims to enhance previous research by\n",
      "conducting a comparative analysis of the performance\n",
      "of various algorithms (SVM, KNN, XGBoost, and Logistic\n",
      "Regression) on the PIMA Indian Diabetes Dataset, and\n",
      "findings suggest that DNN outperforms the other four\n",
      "methods.\n",
      "To evaluate the functioning of our DNN model com-\n",
      "pared to other models, we used accuracy, precision, recall,\n",
      "and F1-score metrics. Our results show that DNNs can\n",
      "achieve a high level of accuracy in predicting diabetes\n",
      "in the PIMA Indian population, outperforming SVM, KNN\n",
      "and NB algorithms. These findings suggest that DNNs can\n",
      "be a valuable tool for predicting and diagnosing diabetes\n",
      "in other populations as well.\n",
      "The results of this research work show that DNNs can\n",
      "achieve a high level of accuracy in predicting diabetes in\n",
      "the PIMA Indian population, surpassing the performance\n",
      "of SVM and KNN algorithms. The model achieves an\n",
      "accuracy of 90% by employing DNNs in TensorFlow, an\n",
      "open-source library for data analysis and ML.\n",
      "Several studies have shown the effectiveness of DNNs\n",
      "in medical diagnosis and prediction tasks. For example,\n",
      "Dong et al. (2020) conducted a systematic review of the\n",
      "application of ML methods in diabetes prediction and\n",
      "found that DNNs are one of the most effective techniques\n",
      "for achieving high levels of accuracy [3]. In a compre-\n",
      "hensive review of deep learning techniques for medical\n",
      "diagnosis, Hassan et al. (2020) also reported the effec-\n",
      "tiveness of DNNs in various medical domains, including\n",
      "diabetes prediction [6]. Al-Turjman et al. (2019) reviewed\n",
      "intelligent health systems for the prediction and diagnosis\n",
      "of diabetes and found that DNNs are among the most\n",
      "promising techniques for achieving high accuracy [5].\n",
      "Overall, the findings of this research work suggest that\n",
      "DNNs can be an effective tool for predicting and diagnos-\n",
      "ing diabetes in other populations as well. The use of DNNs\n",
      "in medical diagnosis and prediction tasks is a promising\n",
      "area of research that can significantly improve patient\n",
      "outcomes and prevent long-term complications associated\n",
      "with chronic diseases like diabetes.\n",
      "This paper is organized as follows. Section 2 presents\n",
      "the related literary research on the proposed study. Sec-\n",
      "tion 3 presents the background and methodology of the\n",
      "proposed work, the dataset used, algorithms, and results.\n",
      "Section 4 analyzes the results and discussion. Section 5\n",
      "finally concludes the study.\n",
      "II. LITERATURE SURVEY\n",
      "A. Background\n",
      "Machine Learning (ML) is increasingly popular area\n",
      "of development in the field of healthcare. ML provides\n",
      "systems that learn from data and improve their behavior\n",
      "over time by automatically discovering emerging patterns\n",
      "from training datasets. The use of ML in healthcare can\n",
      "help improve patient outcomes and prevent long-term\n",
      "complications associated with chronic diseases like dia-\n",
      "betes. In this study, we have focused on using supervised\n",
      "learning models for the classification of diabetes risk\n",
      "prediction. The most common classification methods used\n",
      "in this study include Logistic Regression, Support Vector\n",
      "Machines (SVM), K Nearest Neighbours (KNN), XGBoost\n",
      "and Deep Neural Networks (DNN). In particular, we are\n",
      "interested in comparing the performance of these algo-\n",
      "rithms with Deep Neural Networks (DNNs) for predicting\n",
      "diabetes in the PIMA Indian population. Our study aims to\n",
      "determine if DNNs can achieve a higher level of accuracy\n",
      "in predicting diabetes than existing state-of-the-art algo-\n",
      "rithms like SVM, KNN, XGBoost, and Logistic Regression.\n",
      "The results of this study provides us with valuable insights\n",
      "into the use of DNNs for medical diagnosis and predic-\n",
      "tion tasks. They may lead to significant improvements in\n",
      "patient outcomes and healthcare in general.\n",
      "B. Previous Studies\n",
      "The PIMA Indian Diabetes Dataset [7] has been the\n",
      "subject of much research in the field of machine learning,\n",
      "with various methods proposed to achieve accurate clas-\n",
      "sification results. Deng and Kasabov [8] employed cross-\n",
      "validation and Self-Organizing Maps (SOM) to achieve\n",
      "78.4% accuracy in their classification of the PIMA dataset.\n",
      "Yu et al. [9] explored a combination of methods, including\n",
      "Quantum Particle Swarm Optimization (QPSO), Weighted\n",
      "Least Square (WLS) Support Vector Machine (SVM), and\n",
      "Neural Network, to achieve a classification accuracy of up\n",
      "to 82.18% with the WLS-SVM method. Al Jarullah et al.\n",
      "[10] utilized the c4.5 algorithm and achieved an accuracy\n",
      "of 71.1\n",
      "Pasi Lukka [11] used a combination of feature selection\n",
      "methods based on fuzzy entropy measures and similarity\n",
      "classifiers, achieving a classification accuracy of 75.29%.\n",
      "Seera et al. [12] proposed a hybrid intelligent system that\n",
      "combines the Fuzzy Min-Max neural network, decision\n",
      "tree, and Random Forest model. This approach achieved\n",
      "71.35% accuracy and provided incremental learning fea-\n",
      "tures by incorporating the neural network model and the\n",
      "ability to explain the decision process by incorporating\n",
      "the decision tree.\n",
      "In another paper, Choubey [13] proposed an approach\n",
      "using Genetic Algorithm (GA) for feature selection and\n",
      "Naive Bayes (NB) for classification, achieving an accuracy\n",
      "of 78.69%. . Kumari et al. [14] applied the SVM model for\n",
      "PIMA classification and investigated the performance of\n",
      "various kernels in their experiments. The paper reported\n",
      "a classification accuracy of 75.50% using RBF kernel\n",
      "and cross-validation method to tune the SVM hyper-\n",
      "parameters. Somu et al. [15] introduced RSKHT (Rough\n",
      "Set based KHelly) feature selection technique and com-\n",
      "bined it with the Random Forest classification method,\n",
      "achieving 75.02%, 73.11%, 75.11%, and 74.9% accuracy in\n",
      "their experiments with Random Forest, Bayesian Network,\n",
      "Neural Network, and Decision Trees respectively.\n",
      "A comparative study of these methods reveals that they\n",
      "achieve varying degrees of accuracy in PIMA classification.\n",
      "While Yu et al. [9] achieved the highest accuracy of 82.18%,\n",
      "other approaches such as those of Choubey et al. [13]\n",
      "using GA feature selection achieved an accuracy of 78.69%.\n",
      "Meanwhile, Seera et al. [12] achieved 71.35% accuracy\n",
      "using a hybrid intelligent system, and Al Jarullah et al. [10]\n",
      "reported an accuracy of 71.1% using the c4.5 algorithm.\n",
      "Overall, the use of machine learning methods such\n",
      "as SVM, Random Forest, decision trees, and the neural\n",
      "network has proven effective in PIMA classification. While\n",
      "each method has its own strengths and weaknesses, the\n",
      "development of hybrid intelligent systems and the appli-\n",
      "cation of feature selection techniques have shown promise\n",
      "in achieving even greater accuracy in the future.\n",
      "III. METHODOLOGY\n",
      "A. PIMA Indians Diabetes Dataset\n",
      "1) Data\n",
      "Description:\n",
      "The\n",
      "PIMA\n",
      "Indians\n",
      "Diabetes\n",
      "dataset (PIDD) [7] contains 8 features, such as Pregnancy\n",
      "History, Glucose Levels, Blood Pressure, Skin Thickness,\n",
      "Insulin, BMI, Diabetic Pedigree Function, and Age. There\n",
      "are more instances of non-diabetic (labeled 0) individuals\n",
      "(500) than diabetic (labeled 1) ones (268) as shown in\n",
      "Figure 1. Sample data from the Pima Indians Dataset has\n",
      "been shown in Figure 2.\n",
      "Fig. 1.\n",
      "Outcomes Distribution in the dataset\n",
      "2) Data Cleaning: One of the biggest challenges with\n",
      "this dataset, is the presence of missing data in the Glucose,\n",
      "Blood Pressure, Skin Thickness, Insulin and BMI features.\n",
      "To combat this issue, this study uses the following steps:\n",
      " Step 1: Replace the value with NaN for each feature,\n",
      "to indicate the presence of a missing value.\n",
      " Step 2: Calculate the median value by outcome for\n",
      "each feature, and replace NaN with this median value.\n",
      " Step 3: Check the values of all features after the above\n",
      "adjustment, to ensure the completeness of data.\n",
      "3) Feature Engineering: In this study, we create new\n",
      "features to result in faster training and more accurate\n",
      "predictions while building a model. The following features\n",
      "were newly created after analyzing the dataset:\n",
      "N0, N8, N12, N13, and N14 are features that have been\n",
      "created, to boost the relevant features in the dataset. All\n",
      "the other features have been created, by plotting each\n",
      "feature and figuring out a general trend from the plot.\n",
      "4) Data Standardization: After the above process, it is\n",
      "essential to standardize the dataset as it is a common\n",
      "requirement for various machine learning estimators to\n",
      "perform well for the given dataset. This study utilized\n",
      "the Standard Scaler algorithm to standardize the data, to\n",
      "evaluate the accuracy of different algorithms. The dataset\n",
      "after standardization has been presented in Figure 3.\n",
      "B. ML Techniques for Prediction of Diabetes\n",
      "In this section, different supervised learning methods\n",
      "are presented for classifying Pima Indian women as either\n",
      "diabetic or non-diabetic. These methods use the original\n",
      "dataset to create separate training and testing datasets to\n",
      "accurately classify or predict diabetes.\n",
      "1) Support Vector Machine: Support Vector Machines\n",
      "(SVMs) have been extensively researched and studied in\n",
      "the field of machine learning. The original idea behind\n",
      "SVMs was introduced by Vapnik and Chervonenkis in 1963\n",
      "[16]. The modern version of SVMs, which incorporates the\n",
      "kernel trick, was developed by Boser, Guyon, and Vapnik\n",
      "in 1992 [17]. Since then, SVMs have been widely used\n",
      "in various applications such as image classification, text\n",
      "classification, and bioinformatics.\n",
      "The concept of the kernel trick was initially proposed by\n",
      "Aizerman et al. in 1964 [18], and it was later formalized\n",
      "by Mercer in 1909 [19]. The kernel trick enables SVMs\n",
      "to implicitly map the data to be analysed into a high-\n",
      "dimensional feature space where linear separation is pos-\n",
      "sible.\n",
      "In summary, SVMs are a powerful and widely used\n",
      "machine\n",
      "learning\n",
      "technique\n",
      "that\n",
      "can\n",
      "handle\n",
      "high-\n",
      "dimensional and non-linear datasets while being less\n",
      "susceptible to overfitting.\n",
      "2) K-Nearest Neighbours: K-nearest neighbors (KNN) is\n",
      "a non-parametric algorithm used for classification and\n",
      "regression tasks. It works by finding the K-nearest neigh-\n",
      "bors of a data point and assigning it to the class with\n",
      "the majority vote among those neighbors. In other words,\n",
      "KNN identifies the K closest data points in the training\n",
      "set to a given test point and then classifies the test point\n",
      "by the most common class among its K nearest neighbors\n",
      "[20].\n",
      "KNN is a powerful algorithm for classification tasks,\n",
      "particularly when the decision boundary is complex or\n",
      "the data is not linearly separable [21]. KNN is considered\n",
      "a lazy learner, as it does not work by making assumptions\n",
      "on the underlying data distribution and instead relies on\n",
      "the training data for classification. [22]\n",
      "Overall, KNN is a useful algorithm for classification\n",
      "tasks, particularly when the decision boundary is complex\n",
      "or the data is not linearly separable. Its simplicity and\n",
      "interpretability make it a popular choice among data\n",
      "scientists, though it does have some limitations that\n",
      "should be considered when applying the algorithm to\n",
      "real-world datasets.\n",
      "3) XGBoost:\n",
      "XGBoost is a popular machine-learning\n",
      "algorithm that has gained significant attention and is\n",
      "considered as a state-of-the-art algorithm for classification\n",
      "and regression tasks. It is based on the gradient boosting\n",
      "decision tree (GBDT) algorithm [23]. XGBoost enhances\n",
      "the performance of GBDT by employing a variety of tech-\n",
      "niques such as parallel computing on a single machine,\n",
      "effective regularization, and handling missing values [24].\n",
      "One of the main strengths of XGBoost is its ability to\n",
      "handle high-dimensional data, which is a common char-\n",
      "acteristic of many real-world datasets. In particular, it has\n",
      "been shown to perform well on image and text data [25].\n",
      "The algorithm also employs an effective regularization\n",
      "technique that reduces overfitting, which is a common\n",
      "problem with high-dimensional data. This technique is\n",
      "based on L1 and L2 regularization and can be controlled\n",
      "by adjusting the hyperparameters of the model [24].\n",
      "Another advantage of XGBoost is its ability to handle\n",
      "missing values. This is particularly useful in medical\n",
      "datasets like the PIMA dataset, where missing values are\n",
      "common. XGBoost uses a technique called approximate\n",
      "greedy coordinate descent to impute missing values [24].\n",
      "In summary, XGBoost is a powerful machine-learning\n",
      "Fig. 2.\n",
      "Pima Indians Diabetes Dataset\n",
      "Fig. 3.\n",
      "Dataset after Feature Engineering and Standardization\n",
      "algorithm that can handle high-dimensional data, has ef-\n",
      "fective regularization techniques, and can handle missing\n",
      "values. Its ability to perform parallel computing on a\n",
      "single machine and its compatibility with feature selection\n",
      "techniques and other algorithms make it a suitable choice\n",
      "for many real-world classification and regression tasks.\n",
      "4) Logistic Regression: Logistic regression is a popular\n",
      "machine learning algorithm used for binary classification\n",
      "tasks. It is a statistical model that uses a logistic function\n",
      "to model the probability of a binary response variable\n",
      "based on one or more predictor variables [26]. Logistic\n",
      "regression has been widely used in various applications,\n",
      "including finance, healthcare, and marketing, due to its\n",
      "simplicity, interpretability, and effectiveness [27].\n",
      "One of the strengths of logistic regression is its ability\n",
      "to handle both continuous and categorical predictor vari-\n",
      "ables. Moreover, logistic regression provides interpretable\n",
      "coefficients, which can be used to understand the im-\n",
      "portance of each predictor variable in the model. This\n",
      "interpretability also enables domain experts to validate the\n",
      "model and make informed decisions based on the model\n",
      "outputs. [27]\n",
      "One major advantage of using Logistic Regression is\n",
      "that it can handle high-dimensional data and perform\n",
      "well in sparse datasets [28]. This makes logistic regression\n",
      "a suitable choice for problems with a large number of\n",
      "features or when the dataset has missing values.\n",
      "Furthermore, logistic regression can be easily extended\n",
      "to handle multi-class classification tasks using techniques\n",
      "such as one-vs-all and softmax regression [29]. In addition,\n",
      "logistic regression can be combined with other ML algo-\n",
      "rithms such as decision trees and support vector machines\n",
      "to improve the models performance [30].\n",
      "It can be used as a baseline algorithm for the PIMA\n",
      "Indian Diabetes Dataset, and its performance can be\n",
      "further improved by combining it with feature selection\n",
      "techniques or other machine learning algorithms.\n",
      "C. Proposed Method\n",
      "Artificial Neural Network (ANN) is a model that is\n",
      "inspired by the functioning and structure of biological\n",
      "neurons. A neural network is a connection of multiple\n",
      "neurons connected as the human brain is a connection\n",
      "of 86 billion biological neurons. Along with this, an ANN\n",
      "consists of one or more hidden layers that process the\n",
      "information through neurons and each node works as\n",
      "an activation node; it classifies the outcome of artificial\n",
      "neurons for a better outcome.\n",
      "A dense neural network is a type of artificial neural\n",
      "network where each neuron is connected to every neuron\n",
      "in the adjacent layers. In other words, all the neurons\n",
      "are connected from one layer to the next layer. Each\n",
      "connection between neurons has a corresponding weight,\n",
      "which determines the strength of the connection. [31]\n",
      "Each neuron in a neural network has an activation\n",
      "function, which is used to determine the importance of an\n",
      "output from the neuron. This paper uses the Parametric\n",
      "Rectified Linear Unit (PReLU) and Sigmoid activation\n",
      "function. PReLU is a modification of the Rectified Linear\n",
      "Unit (ReLU) activation which sets all negative values to\n",
      "zero and leaves positive values unchanged. PReLU intro-\n",
      "duces a parameter that allows it to adjust the slope of\n",
      "the function for negative inputs. This means that PReLU\n",
      "can learn the appropriate slope for negative inputs during\n",
      "training. The equation of PReLU is described in 1.\n",
      "samples. The F1 score is interpreted as the harmonic\n",
      "mean of precision and recall, and an F1 score has its best\n",
      "value at 1 and the worst score at 0.\n",
      "f (x) =\n",
      "x,\n",
      "x > 0\n",
      "x,\n",
      "x  0\n",
      "(1)\n",
      "Accuracy =\n",
      "T P +T N\n",
      "T P +T N +FP +F N\n",
      "(4)\n",
      "Precision =\n",
      "T P\n",
      "T P +FP\n",
      "(5)\n",
      "where  is the learnable parameter that determines the\n",
      "slope for negative inputs and x is the input feature.\n",
      "The Sigmoid activation is a mathematical function that\n",
      "maps any input value to a value between 0 and 1. It is also\n",
      "a non-linear function, which allows neural networks to\n",
      "model complex relationships between inputs and outputs.\n",
      "The function has been described in 2, where x is the input\n",
      "feature.\n",
      "f (x) =\n",
      "1\n",
      "1+ex\n",
      "(2)\n",
      "where  is the learnable parameter that determines the\n",
      "slope for negative inputs and x is the input feature.\n",
      "The Sigmoid activation is a mathematical function that\n",
      "maps any input value to a value between 0 and 1. It is also\n",
      "a non-linear function, which allows neural networks to\n",
      "model complex relationships between inputs and outputs.\n",
      "The function has been described in 2, where x is the input\n",
      "feature.\n",
      "f (x) =\n",
      "1\n",
      "1+ex\n",
      "(2)\n",
      "Recall =\n",
      "T P\n",
      "T P +F N\n",
      "(6)\n",
      "F1 = 2Precision Recall\n",
      "Precision +Recall\n",
      "(7)\n",
      "where TP = True Positives, FP = False Positives, TN = True\n",
      "Negatives and FN = False Negatives\n",
      "IV. RESULTS\n",
      "In this paper, we compared the performance of five\n",
      "classification algorithms (XGBoost, Logistic Regression,\n",
      "KNN, SVM and DNN) in predicting diabetes using the\n",
      "PIMA dataset. The dataset has been split into 70% training\n",
      "data and 30% testing data, and used for model evalutation.\n",
      "The DNN has been trained with the BinaryCrossentropy\n",
      "Loss Function and uses the Adam Optimizer for finding\n",
      "an optimal learning rate.\n",
      "Our results showed that the DNN outperformed the\n",
      "other methods, achieving the highest accuracy of 91.34%.\n",
      "The second-best method was XGBoost, with an accu-\n",
      "racy of 89.61%. Logistic Regression achieved an accuracy\n",
      "of 83.55%, while KNN and SVM achieved accuracies of\n",
      "80.08% and 79.65%, respectively.\n",
      "TABLE I\n",
      "ACCURACY, PRECISION AND RECALL COMPARISON OF DIFFERENT METHODS\n",
      "The superior performance of DNN can be attributed to\n",
      "its ability to learn complex and nonlinear relationships\n",
      "between features and target variables. DNN can auto-\n",
      "matically extract high-level features from raw input data\n",
      "and use them to make accurate predictions. In contrast,\n",
      "the other methods we tested rely on linear relationships\n",
      "between features and target variables, which may not\n",
      "be sufficient for capturing the complexity of the PIMA\n",
      "dataset.\n",
      "V. CONCLUSIONS\n",
      "In conclusion, the results of this study illustrate the\n",
      "potential of using DNNs for predicting diabetes in the\n",
      "PIMA Indian population. Our findings indicate that DNNs\n",
      "outperform other traditional algorithms such as SVM,\n",
      "KNN, XGBoost, and Logistic Regression in terms of accu-\n",
      "racy, precision, recall, and F1-score metrics. The accuracy\n",
      "We have also implemented Lasso Regression (Least\n",
      "Absolute Shrinkage and Selection Operator) or L1 regu-\n",
      "larization which adds the absolute value of magnitude\n",
      "coefficient as a regularization term to the loss function\n",
      "to avoid underfitting. The coefficient can be calculated\n",
      "using the function described in 3. Another merit of Lasso\n",
      "Regression is that it shrinks the less significant parameters\n",
      "to zero to train the model with the most important\n",
      "parameters.\n",
      "Loss = Error(Y  Y )+\n",
      "n\n",
      "1\n",
      "|wi|\n",
      "(3)\n",
      "where Y is the actual label, Y is the predicted label, \n",
      "is the regularization parameter.\n",
      "In this paper, we propose a Dense Neural Network\n",
      "consisting of an input layer, 2 Dense layers with a PReLU\n",
      "activation function and a final output layer with a Sigmoid\n",
      "activation function. The input layer consists of 24 neurons,\n",
      "the first dense layer consists of 100 neurons, the second\n",
      "dense layer consists of 50 neurons and the output layer\n",
      "has a single neuron. The overall architecture of the neural\n",
      "network can be found in Fig. 4. The L1 regularization\n",
      "technique (Lasso Regression) has been implemented to\n",
      "create a less complex model and to address over-fitting\n",
      "issues. A flowchart of our proposed method has been\n",
      "presented in Fig. 5.\n",
      "D. Performance Evaluation\n",
      "To evaluate different models, we will be using the\n",
      "Accuracy, Precision, Recall and F1 metrics. Accuracy can\n",
      "be defined as the percentage of correct predictions done\n",
      "by the model. It can also be defined as the ratio between\n",
      "the sum of True Positives (TP) and True Negatives (TN) to\n",
      "the count of the entire dataset. Precision can be defined as\n",
      "the ratio of correctly classified positive samples (true pos-\n",
      "itives) to the total number of classified positive samples.\n",
      "The recall is calculated by the ratio between the number\n",
      "of positive samples (true positives) correctly classified\n",
      "to the total number of positive samples (true positives).\n",
      "The recall measures the models ability to detect positive\n",
      "Fig. 4.\n",
      "DNN Architecture\n",
      "Fig. 5.\n",
      "Flowchart of our Proposed Model\n",
      "of the DNN model achieved in this study (91.34%) is highly\n",
      "promising and can be used as a tool for predicting and\n",
      "diagnosing diabetes in other populations as well.\n",
      "The utilization of DNNs proves to be a powerful ap-\n",
      "proach in the field of ML and data analysis. However, it\n",
      "is imperative that there is always room for further im-\n",
      "provement and optimization of the DNN model to achieve\n",
      "even higher levels of accuracy. Additionally, the ethical\n",
      "implications of using predictive models in healthcare and\n",
      "ensure that these models do not perpetuate biases or\n",
      "discrimination.\n",
      "In conclusion, this study contributes to the growing\n",
      "body of research on the application of ML techniques\n",
      "in healthcare and provides evidence for the potential of\n",
      "DNNs as a tool for predicting and diagnosing diabetes.\n",
      "Future research could explore the use of DNNs in other\n",
      "healthcare domains and investigate the impact of such\n",
      "models on clinical decision-making and patient outcomes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_tables_text = []\n",
    "all_text = []\n",
    "\n",
    "mode = 2\n",
    "\n",
    "for page in doc:\n",
    "    # Find tables and associated text\n",
    "    table_finder = page.find_tables()\n",
    "    for table in table_finder.tables:\n",
    "        table_text = page.get_text(clip=table.bbox)\n",
    "        all_tables_text.append(table_text)\n",
    "    \n",
    "    # Single col research paper\n",
    "    if (mode == 1):\n",
    "        text = page.get_text(sort=True)\n",
    "        all_text.append(text)\n",
    "    \n",
    "    # Multi col research paper\n",
    "    elif (mode == 2):\n",
    "        bboxes = column_boxes(page, no_image_text=True)\n",
    "        for rect in bboxes:\n",
    "            text = page.get_text(clip=rect, sort=True)\n",
    "            # text = text.replace('\\n', ' ') + '\\n'\n",
    "            print ('bro', text)\n",
    "            all_text.append(text)\n",
    "\n",
    "# print (all_text)\n",
    "# Remove tables\n",
    "full_text = ''.join(all_text)\n",
    "for table_text in all_tables_text:\n",
    "    if (table_text in full_text):\n",
    "        full_text = full_text.replace(table_text, '')\n",
    "\n",
    "# Remove references\n",
    "pos = re.findall('references', full_text, flags=re.IGNORECASE)\n",
    "exact_word = ''\n",
    "for ind in range(len(pos)-1, -1, -1):\n",
    "    if (pos[ind] == 'REFERENCES'):\n",
    "        exact_word = 'REFERENCES'\n",
    "        break\n",
    "    if (pos[ind] == 'References'):\n",
    "        exact_word = 'References'\n",
    "        break\n",
    "\n",
    "word_ind = full_text.rfind(exact_word)\n",
    "full_text = full_text[:word_ind]\n",
    "\n",
    "print (full_text.encode('ascii', errors='ignore').decode())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
